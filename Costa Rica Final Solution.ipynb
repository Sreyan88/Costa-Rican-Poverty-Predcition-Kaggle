{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sreyan\\Miniconda3\\envs\\AI\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4940ac7103cd208ff9d54cec6c3e6f8fbce949a3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# this only transforms the idhogar field, the other things this function used to do are done elsewhere\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "# plot feature importance for sklearn decision trees    \n",
    "def feature_importance(forest, X_train):\n",
    "    ranked_list = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "    \n",
    "    return ranked_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6758f3d18718290dde4e2db08d7f3177ce58ff0e"
   },
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "                'escolari': ['min', 'max', 'mean']\n",
    "               }\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "\n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "\n",
    "    # Drop id's\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2143a9c7c063af6c8ac6875dc975bfbbae948a2"
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # encode the idhogar\n",
    "    encode_data(df_)\n",
    "    \n",
    "    # create aggregate features\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9746d63e5026548e476d64e70230a67f38dbda5"
   },
   "outputs": [],
   "source": [
    "tribal = []\n",
    "\n",
    "for i, row in train.iterrows():\n",
    "    \n",
    "    if row['lugar2'] == 1:\n",
    "        tribal.append(1)\n",
    "    elif row['lugar4'] == 1:\n",
    "        tribal.append(1)\n",
    "    else:\n",
    "        tribal.append(0)\n",
    "        \n",
    "train['tribal'] = tribal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f8b2d7ef58024809699e1d979f7c6062b723076"
   },
   "outputs": [],
   "source": [
    "tribal = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    \n",
    "    if row['lugar2'] == 1:\n",
    "        tribal.append(1)\n",
    "    elif row['lugar4'] == 1:\n",
    "        tribal.append(1)\n",
    "    else:\n",
    "        tribal.append(0)\n",
    "        \n",
    "test['tribal'] = tribal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00b96f5ad5ab130a49be6dc91e56d51b6e923b67"
   },
   "outputs": [],
   "source": [
    "# some dependencies are Na, fill those with the square root of the square\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "# fill \"no\"s for education with 0s\n",
    "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "\n",
    "# if education is \"yes\" and person is head of household, fill with escolari\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "# convert to int for our models\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "# create feature with max education of either head of household\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
    "\n",
    "# fill some nas\n",
    "train['v2a1']=train['v2a1'].fillna(0)\n",
    "test['v2a1']=test['v2a1'].fillna(0)\n",
    "\n",
    "test['v18q1']=test['v18q1'].fillna(0)\n",
    "train['v18q1']=train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[pd.isnull(train['meaneduc']), 'meaneduc'] = train.loc[pd.isnull(train['meaneduc']), 'escolari']\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[pd.isnull(test['meaneduc']), 'meaneduc'] = test.loc[pd.isnull(test['meaneduc']), 'escolari']\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet, \n",
    "# if there is no water we'll assume they do not\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e803b490ab18612c9d37c8f7f3dc834b47b01ca9"
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c73ce25f137a87233e2e0af2d2a5eb61429bbb88"
   },
   "outputs": [],
   "source": [
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        #deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # dummy colmn name to be added\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23f5a71cd0a31dc2e5534310e16560cdc652c23c"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f52dace92f3d818c198d77116af21ba04cef97e9"
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "# add some aggregates by geography\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e03e901163730a9cc9dd03b5d0d313fc58b94f2"
   },
   "outputs": [],
   "source": [
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "# add some extra features, these were taken from another kernel\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "    # some households have no one over 18, use the total rent for those\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)    \n",
    "extract_features(test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b3dc0008be595e0bb78bb921f4357d8528ee5f8"
   },
   "outputs": [],
   "source": [
    "womenpoor = []\n",
    "\n",
    "for i, row in train.iterrows():\n",
    "    if row['parentesco1']==1 and row['female']==1:\n",
    "        womenpoor.append(1)\n",
    "    else:\n",
    "        womenpoor.append(0)\n",
    "        \n",
    "train['womenpoor'] = womenpoor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "151b75e58b8857eff32ecadd8aa81b391a3bff25"
   },
   "outputs": [],
   "source": [
    "womenpoor = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    if row['parentesco1']==1 and row['female']==1:\n",
    "        womenpoor.append(1)\n",
    "    else:\n",
    "        womenpoor.append(0)\n",
    "        \n",
    "test['womenpoor'] = womenpoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1acfebc2f95ada5a20ca0f4cef5c75eb3a2935b7"
   },
   "outputs": [],
   "source": [
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female', ]\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "192987de1895e80675a0c6f4368dbf80190c3650"
   },
   "outputs": [],
   "source": [
    "corr_matrix = train.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.975)]\n",
    "\n",
    "print(f'There are {len(to_drop)} correlated columns to remove.')\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f23283bd74cf274f3321bfc2473851ebc6a347c"
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns = to_drop)\n",
    "test = test.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5641125ff2e58e10f07202435c55275f688b83b"
   },
   "outputs": [],
   "source": [
    "def split_data(train, y, households, test_percentage=0.20, seed=None):\n",
    "    # uncomment for extra randomness\n",
    "#     np.random.seed(seed=seed)\n",
    "    \n",
    "    train2 = train.copy()\n",
    "    \n",
    "    # pick some random households to use for the test data\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "    \n",
    "    # select households which are in the random selection\n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96131a05d526dd6a9571d40a5dbfa6ef9e6f8f54"
   },
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "# X = train.copy()\n",
    "\n",
    "# pull out and drop the target variable\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=1977)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "# train on entire dataset\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b0cdbe3e445793b918d8efae915dfd1728f82ba"
   },
   "outputs": [],
   "source": [
    "extra_drop_features = [\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'agg18_estadocivil5_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'geo_pared_LE_5',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'geo_pared_LE_4',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'num_over_18',\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil4_COUNT',\n",
    " 'r4t3_to_tamhog',\n",
    " 'agg18_estadocivil3_COUNT',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_6',\n",
    " 'geo_pared_LE_0',\n",
    " 'geo_pared_LE_7',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco10_MEAN',\n",
    " 'agg18_parentesco8_MEAN',\n",
    " 'rez_esc',\n",
    " 'hacdor',\n",
    " 'hacapo',\n",
    " 'fe_people_weird_stat',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_eviv_LE_0',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_epared_LE_2',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_0',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'parentesco_LE',\n",
    " 'agg18_parentesco6_MEAN',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco4_MEAN',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco12_MEAN',\n",
    " 'agg18_parentesco12_COUNT',\n",
    "  ] + ['geo_hogar_nin', 'geo_manual_elec_LE_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bc84065688c4fc008386e43ac6029eded82e3a6"
   },
   "outputs": [],
   "source": [
    "lol=[item for item in extra_drop_features if item not in to_drop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43878492fa10f5f506d9e1f68d45d70257099dad"
   },
   "outputs": [],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93642a4583519bfb046483878d172a13ca554e72"
   },
   "outputs": [],
   "source": [
    "lgb_drop_cols = lol + [\"idhogar\",  'parentesco1']\n",
    "try:\n",
    "    X_train.drop(lgb_drop_cols, axis=1, inplace=True)\n",
    "    X_test.drop(lgb_drop_cols, axis=1, inplace=True)\n",
    "except:\n",
    "    print(\"Error dropping columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6ae95e858cd463b8e2448bd19728b73a5aa037c"
   },
   "outputs": [],
   "source": [
    "opt_parameters = {'colsample_bytree': 0.88, 'min_child_samples': 90, 'num_leaves': 16, 'subsample': 0.94, 'reg_lambda': 0.5, }\n",
    "opt_parameters = {'colsample_bytree': 0.88, 'min_child_samples': 95, 'num_leaves': 25, 'subsample': 0.94, 'reg_lambda': 0.5, }\n",
    "\n",
    "def evaluate_macroF1_lgb(truth, predictions):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', f1, True) \n",
    "\n",
    "fit_params={\"early_stopping_rounds\":800,\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'eval_names': ['train', 'valid'],\n",
    "            'verbose': False,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 200\n",
    "fit_params['callbacks'] = [lgb.reset_parameter(learning_rate=learning_rate_power_0997)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "751a1f9a11ff65b4813d891c3a0490b04df6a9e5"
   },
   "outputs": [],
   "source": [
    "def _parallel_fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n",
    "    \n",
    "    # randomly split the data so we have a test set for early stopping\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, households=train_households, seed=1972)\n",
    "    \n",
    "    # update the fit params with our new split\n",
    "    fit_params[\"eval_set\"] = [(X_train,y_train), (X_test,y_test)]\n",
    "    \n",
    "    # fit the estimator\n",
    "    if sample_weight is not None:\n",
    "        estimator.fit(X_train, y_train, sample_weight=sample_weight, **fit_params)\n",
    "    else:\n",
    "        estimator.fit(X_train, y_train, **fit_params)\n",
    "    return estimator\n",
    "\n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    '''\n",
    "    This implements the fit method of the VotingClassifier propagating fit_params\n",
    "    '''\n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            for name, step in self.estimators:\n",
    "                if not has_fit_parameter(step, 'sample_weight'):\n",
    "                    raise ValueError('Underlying estimator \\'%s\\' does not'\n",
    "                                     ' support sample weights.' % name)\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40eb775b186a0ddc9e9dd600ab7443e3d7dd15c5"
   },
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for i in range(30):\n",
    "    clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n",
    "                             random_state=314+i, silent=True, metric='None', \n",
    "                             n_jobs=4, n_estimators=5000,min_child_weight=48.94067592560281,class_weight='balanced')\n",
    "    \n",
    "    clf.set_params(**opt_parameters)\n",
    "    clfs.append(('lgbm{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del clfs\n",
    "\n",
    "#Train the final model with learning rate decay\n",
    "_ = vc.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b5a2e750cc92283c1d534858df6367bd5cfaf57"
   },
   "outputs": [],
   "source": [
    "global_score = f1_score(y_test, clf_final.predict(X_test), average='macro')\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test), average='macro')\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbc059bd0f39c921149ef64ed2bc350f05009fdf"
   },
   "outputs": [],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d312cb669d32aaf440ba85b1b8afe53ff30c632b"
   },
   "outputs": [],
   "source": [
    "extra_drop_features = ranked_features[116:]\n",
    "extra_drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "252f73396be9ae90f2530b4f65633af13f86e469"
   },
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e21cf95510a0d9ae5ae6d4ac1b1d04ff79fe8664"
   },
   "outputs": [],
   "source": [
    "y_subm['Target'] = clf_final.predict(test[X_train.columns]) + 1\n",
    "\n",
    "vc.voting = 'soft'\n",
    "y_subm_soft = y_subm.copy(deep=True)\n",
    "y_subm_soft['Target'] = vc.predict(test[X_train.columns]) + 1\n",
    "\n",
    "vc.voting = 'hard'\n",
    "y_subm_hard = y_subm.copy(deep=True)\n",
    "y_subm_hard['Target'] = vc.predict(test[X_train.columns]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c3cf07fe3f3c157998c982c56f8d0220d8c5c03"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file = 'submission_LGB_{:.4f}_{}.csv'.format(global_score, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_soft = 'submission_soft_LGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_hard = 'submission_hard_LGB_{:.4f}_{}.csv'.format(global_score_hard, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_0forNonHeads = 'submission_0forNonHead_LGB_{:.4f}_{}.csv'.format(global_score, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "\n",
    "y_subm_soft.to_csv(sub_file_soft, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f23eb05d5e8550e7e05d138e85997222c410390b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
